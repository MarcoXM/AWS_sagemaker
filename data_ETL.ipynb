{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data ETL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIyfETPhjNZPR7IIxOxzT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoXM/AWS_sagemaker/blob/master/data_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyTVYYK5PZS6"
      },
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "similarweb_api_key = \"55a55871a3fb4fefad9fbc06c5dfe9a1\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_nums_visit(domain, date_start = \"2018-11\", date_end = \"2020-11\", granularity = \"monthly\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/total-traffic-and-engagement/visits?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&granularity={granularity}&main_domain_only=false&format=json\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "def get_bounce_rate(domain,date_start = \"2018-11\", date_end = \"2020-11\",granularity = \"monthly\"):\n",
        "\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/total-traffic-and-engagement/bounce-rate?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&granularity={granularity}&main_domain_only=false&format=json\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "def get_total_visit_duration(domain,date_start = \"2018-11\", date_end = \"2020-11\",granularity = \"monthly\"):\n",
        "    \n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/total-traffic-and-engagement/average-visit-duration?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&granularity={granularity}&main_domain_only=false&format=json\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "def get_desktop_unique_visiter(domain,date_start = \"2018-11\", date_end = \"2020-11\",granularity = \"monthly\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/unique-visitors/desktop_unique_visitors?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&main_domain_only=false&format=json&granularity={granularity}\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "def get_mobile_unique_visiter(domain,date_start = \"2018-11\", date_end = \"2020-11\",granularity = \"monthly\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/unique-visitors/mobileweb_unique_visitors?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&main_domain_only=false&granularity={granularity}&format=json&show_verified=false\"\n",
        "            \n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "def get_page_per_visit(domain,date_start = \"2018-11\", date_end = \"2020-11\",granularity = \"monthly\"):\n",
        "    #\n",
        "        # return page view per day of the domain  \n",
        "    # \n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/total-traffic-and-engagement/pages-per-visit?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&granularity={granularity}&main_domain_only=false&format=json\"\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "def get_desktop_traffic_sources_overview(domain,date_start = \"2018-11\", date_end = \"2020-11\",granularity = \"monthly\"):\n",
        "\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/traffic-sources/overview-share?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=gb&granularity={granularity}&main_domain_only=false&format=json\"\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_audience_interests(domain,date_start = \"2018-11\", date_end = \"2020-11\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/audience-interests/also-visited?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&main_domain_only=false&format=json\"\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_similiar_site(domain,date_start = \"2018-11\", date_end = \"2020-11\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/similar-sites/similarsites?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&format=json\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_desktop_traffic_sources(domain,date_start = \"2018-11\", date_end = \"2020-11\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/traffic-sources/overview?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&main_domain_only=false&format=json&show_verified=false\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "def get_desktop_traffic_sources_overview(domain,date_start = \"2018-11\", date_end = \"2020-11\", granularity = \"monthly\"):\n",
        "    url = f\"https://api.similarweb.com/v1/website/{domain}/traffic-sources/overview-share?api_key={similarweb_api_key}&start_date={date_start}&end_date={date_end}&country=world&granularity={granularity}&main_domain_only=false&format=json\"\n",
        "\n",
        "    payload = {}\n",
        "    headers= {}\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data = payload)\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "kpi_dict = {\n",
        "    \"Visit\": get_nums_visit,\n",
        "    \"Bounce_Rate\":get_bounce_rate,\n",
        "    \"Visit_Duration\":get_total_visit_duration,\n",
        "    \"Visitor_desktop\": get_desktop_unique_visiter,\n",
        "    \"Visitor_Mobile\":get_mobile_unique_visiter,\n",
        "    \"Page/Visit\":get_page_per_visit,\n",
        "    \"Destop_Traffic_Source\":get_desktop_traffic_sources_overview,\n",
        "    \"Audience_Interests\":get_audience_interests, ## non - times stamp data\n",
        "    \"Similar_Sites\" : get_similiar_site, ## non - times stamp data\n",
        "    \"Desktop_Traffic_Sources\": get_desktop_traffic_sources,\n",
        "    \"Desktop_Traffic_Sources_Overview\":get_desktop_traffic_sources_overview\n",
        "}\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "YgOzvzmwPHAD",
        "outputId": "f3ce6530-b4ce-4b2c-8e43-f4069640a6eb"
      },
      "source": [
        "#### import packages we need\n",
        "\n",
        "########################################################\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import warnings\n",
        "# from config import kpi_dict\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "##########################################################\n",
        "####  Config setting \n",
        "SIMILARWEB_API_KEY  = \"55a55871a3fb4fefad9fbc06c5dfe9a1\"\n",
        "####\n",
        "##########################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def to_df(response, key , col_name = None ):\n",
        "    df = pd.DataFrame(response.get(key))\n",
        "    if col_name is not None:\n",
        "        df.rename(columns={key: col_name }, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "class Similarweb_Dataset(object):\n",
        "    def __init__(self, date_start, date_end , api_key, granularity , kpi_dict):\n",
        "\n",
        "        '''\n",
        "        date_start str\n",
        "        date_end str \n",
        "        format : yyyy-mm\n",
        "        granularity -> daily, monthly and yearly\n",
        "        '''\n",
        "\n",
        "        self.date_start = date_start\n",
        "        self.date_end = date_end\n",
        "        self.api_key = api_key\n",
        "        self.granularity = granularity\n",
        "        self.kpi_list_total = list(kpi_dict.keys())\n",
        "        self.kpi_dict = kpi_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_data_of_websites(self, target_website_lists , kpis_list ):\n",
        "\n",
        "        \"\"\"\n",
        "        target_website_lists : list of str [\n",
        "            \"etsy.com\",\n",
        "            \"farfetch.com\",\n",
        "            \"therealreal.com\",\n",
        "            \"tractorsupply.com\",\n",
        "        ]\n",
        "        return a dataframe of all website result\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        dit = {}\n",
        "       \n",
        "        for web in target_website_lists:\n",
        "            dit[web] = self.get_data_of_one_website(web, kpis_list)\n",
        "\n",
        "        final_raw_data = pd.concat(dit.values()).sort_values(\"date\")\n",
        "\n",
        "        return final_raw_data\n",
        "\n",
        "\n",
        "    def add_percentchange(self, df):\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "        cols = df.select_dtypes(include=numerics).columns\n",
        "        for col in cols:\n",
        "            df[f'{self.granularity}_change_{col}'] = df[col].pct_change()\n",
        "        return df\n",
        "\n",
        "\n",
        "    def get_data_of_one_website(self, domain , kpis_list):\n",
        "        '''\n",
        "        Combine the KPI of one website\n",
        "\n",
        "        @ domain str: website \n",
        "        return data_frame of the KPI\n",
        "        @kpis_list = a list of KPI we need\n",
        "\n",
        "        '''\n",
        "\n",
        "        if not kpis_list:\n",
        "            return \"There should be at least ONE kpi !\"\n",
        "        n = len(kpis_list)\n",
        "        json_response = self.kpi_dict[kpis_list[0]](domain , date_start= self.date_start , date_end= self.date_end, granularity=self.granularity).json()\n",
        "        key = list(json_response.keys())[1]\n",
        "        df = to_df(json_response,key,col_name = kpis_list[0])\n",
        "\n",
        "        if n > 1:\n",
        "            for i in range(1, n):\n",
        "                json_response = self.kpi_dict[kpis_list[i]](domain , date_start= self.date_start , date_end= self.date_end, granularity=self.granularity).json()\n",
        "                key = list(json_response.keys())[1]\n",
        "                df = df.merge(to_df(json_response,key,col_name = kpis_list[i]))\n",
        "\n",
        "        df = self.add_percentchange(df)\n",
        "        df['site'] = domain.split(\".\")[0]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    sw_data_etl = Similarweb_Dataset(\"2018-11\",\"2020-11\",SIMILARWEB_API_KEY, \"monthly\",kpi_dict)\n",
        "\n",
        "\n",
        "    website2ticker = {}\n",
        "    website2Name = {}\n",
        "    website2IndustryGroup = {}\n",
        "\n",
        "    # consumer_companies = pd.read_csv(\"companies_1205.csv\")\n",
        "\n",
        "    # test = consumer_companies.dropna()\n",
        "    # test['Site'] = test['Site'].apply(lambda x : x[12:] if \"www\" in x else x[8:])\n",
        "\n",
        "    # def prune(s):\n",
        "    #     if not s : return s\n",
        "    #     if s[-1] == \"/\":return s[:-1]\n",
        "    #     return s\n",
        "\n",
        "    # for idx, values in test.iterrows():\n",
        "    #     website2ticker[prune(values[3]).split(\".\")[0]] = values[0]\n",
        "    #     website2Name[prune(values[3]).split(\".\")[0]]= values[1]\n",
        "    #     website2IndustryGroup[prune(values[3]).split(\".\")[0]]= values[2]\n",
        "\n",
        "\n",
        "    ## could get the list of target website by read csv file\n",
        "    target_website = website2ticker.keys()\n",
        "    target_websites = [\n",
        "        \"etsy.com\",\n",
        "        \"farfetch.com\",\n",
        "        \"therealreal.com\",\n",
        "        \"tractorsupply.com\",\n",
        "    ]\n",
        "    k_list = ['Visit', 'Bounce_Rate', 'Visit_Duration', 'Visitor_desktop', 'Visitor_Mobile', 'Page/Visit']\n",
        "    \n",
        "\n",
        "    sw_website_data = sw_data_etl.get_data_of_websites(target_websites , k_list)\n",
        "    print(sw_website_data.head())\n",
        "\n",
        "\n",
        "    print(\"raw data saved!!!\")\n",
        "    sw_website_data.to_csv(\"raw_data.csv\",index = False)\n",
        "    ### transform \n",
        "    # new_columns_name = [\"site\",\"year\",\"month\",\"Bounce rate\"，\"Page views\", \"Time on the Platform\", \"Unique Vistors\", \"# of Clicks\", \"M/M change%\",\"Q/Q change%\", \"Y/Y change\", \"Quanter to date\"]\n",
        "    # final_data = transform_data(sw_website_data, new_columns_name)\n",
        "    # final_data['ticker'] = final_data['site'].apply(lambda x: website2ticker[x])\n",
        "    # final_data['Name'] = final_data['site'].apply(lambda x: website2Name[x])\n",
        "    # final_data['IndustryGroup'] = final_data['site'].apply(lambda x: website2IndustryGroup[x])\n",
        "    # final_data = final_data[['ticker', 'Name',\n",
        "    #    'IndustryGroup','site', 'year', 'month', 'Bounce rate', 'Page views',\n",
        "    #    'Time on the Platform', 'Unique Vistors', '# of Clicks', 'M/M change%',\n",
        "    #    'Q/Q change%', 'Y/Y change', 'Quanter to date']]\n",
        "    print(\"final data get and save !!\")\n",
        "    # final_data.to_csv(\"data_processed.csv\",index=False)\n",
        "    print(final_data.head())\n",
        "    print(\"done !!\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         date         Visit  ...  monthly_change_Page/Visit           site\n",
            "0  2018-11-01  2.282811e+08  ...                        NaN           etsy\n",
            "0  2018-11-01  5.477303e+06  ...                        NaN    therealreal\n",
            "0  2018-11-01  1.538683e+07  ...                        NaN  tractorsupply\n",
            "0  2018-11-01  2.048180e+07  ...                        NaN       farfetch\n",
            "1  2018-12-01  1.266044e+07  ...                    0.02197  tractorsupply\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "raw data saved!!!\n",
            "final data get and save !!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e57e981dfec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m#    'Q/Q change%', 'Y/Y change', 'Quanter to date']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final data get and save !!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mfinal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_processed.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done !!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFAIP2qwPX9i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}